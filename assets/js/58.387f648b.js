(window.webpackJsonp=window.webpackJsonp||[]).push([[58],{418:function(s,e,t){"use strict";t.r(e);var a=t(7),v=Object(a.a)({},(function(){var s=this,e=s._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[e("h1",{attrs:{id:"redis"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#redis"}},[s._v("#")]),s._v(" Redis")]),s._v(" "),e("h2",{attrs:{id:"高可用篇"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#高可用篇"}},[s._v("#")]),s._v(" 高可用篇")]),s._v(" "),e("h3",{attrs:{id:"redis如何实现服务高可用"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#redis如何实现服务高可用"}},[s._v("#")]),s._v(" Redis如何实现服务高可用")]),s._v(" "),e("p",[s._v("要想设计一个高可用的Redis服务，一定要从Redis的多服务节点来考虑，比如Redis的主从复制，哨兵模式，切片集群。")]),s._v(" "),e("h4",{attrs:{id:"主从复制"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#主从复制"}},[s._v("#")]),s._v(" 主从复制")]),s._v(" "),e("p",[s._v("主从复制是Redis高可用服务的最基础的保证，实现方案就是将从前的一台Redis服务器，同步数据到多台从Redis服务器上，即一主多从的模式，且从服务器之间采用的是读写分离的方式。")]),s._v(" "),e("p",[s._v("主服务器可以进行读写操作，当发生写操作时自动将写操作同步给从服务器，而从服务器一般是只读，并接受主服务器同步过来的写操作命令，然后执行这条命令。")]),s._v(" "),e("p",[e("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/qouson/my-pic-bed/pic/202406262247029.png",alt:""}})]),s._v(" "),e("p",[s._v("也就是说，所有的数据修改只在主服务器上进行，然后将最新的数据同步给服务器，这样就使得主从服务器的数据是一致的。")]),s._v(" "),e("p",[s._v("注意，主从服务器之间的命令复制时异步进行的。")]),s._v(" "),e("p",[s._v("具体来说，在主服务器命令传播阶段，主服务器接受到新的写命令后，会发送给从服务器。但是，主服务器并不会等从服务器实际执行完命令后，再把结果返回给客户端，而是主服务器自己在本地执行完命令后，就会向客户端返回结果了。如果从服务器还没有执行主服务器同步过来的命令，主从服务器间的数据就不一致了。")]),s._v(" "),e("p",[s._v("所以，无法实现强一致性保证（主从数据时时刻刻保持一致），数据不一致是难以避免的。")]),s._v(" "),e("h4",{attrs:{id:"哨兵模式"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#哨兵模式"}},[s._v("#")]),s._v(" 哨兵模式")]),s._v(" "),e("p",[s._v("在使用Redis主从服务的时候，会有一个问题，就是当Redis的主从服务出现故障宕机时候，需要手动进行恢复。")]),s._v(" "),e("p",[s._v("为了解决这个问题，Redis增加了哨兵模式，因为哨兵模式做到了可以监控主从服务器，并且提供主从节点故障转移的功能。")]),s._v(" "),e("p",[e("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/qouson/my-pic-bed/pic/202406262259668.png",alt:""}})]),s._v(" "),e("h4",{attrs:{id:"切片集群模式"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#切片集群模式"}},[s._v("#")]),s._v(" 切片集群模式")]),s._v(" "),e("p",[s._v("当Redis缓存数据量大到一台服务器无法缓存时，就需要使用Redis切片集群方案，它将数据分布在不同服务器上，一次来降低系统对单主节点的依赖，从而提高Redis服务的读写性能。")]),s._v(" "),e("p",[s._v("Redis Cluster方案采用哈希槽（Hash Slot），来处理数据和节点之间的映射关系。在Redis Cluster方案中，一个切片集群共有16384个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的key，被映射到一个哈希槽中，具体执行过程分为两大步：")]),s._v(" "),e("ul",[e("li",[s._v("根据键值对的key，按照CRC16算法计算一个16bit的值。")]),s._v(" "),e("li",[s._v("再用16bit值对16384取模，得到0~16383范围的模数，每个模数代表一个相应编号的哈希槽。")])]),s._v(" "),e("p",[s._v("接下来问题就是，这些哈希槽怎么被映射到具体的Redis节点上")]),s._v(" "),e("ul",[e("li",[s._v("平均分配：在使用cluster create 命令创建Redis集群时，Redis会自动把所有哈希槽平均分布到集群节点上。比如集群中有9个节点，则每个节点上槽的个数为16384/9个")]),s._v(" "),e("li",[s._v("手动分配：可以使用cluster meet命令手动建立节点间的连接，组成集群，再用cluster addslots命令，指定每个节点上的哈希槽个数。")])]),s._v(" "),e("p",[e("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/qouson/my-pic-bed/pic/202406262312241.png",alt:""}})]),s._v(" "),e("p",[s._v("上图中的切片集群一共有2个节点，假设有4个哈希槽（slot0~slog3）时，我们就可以通过命令手动分配哈希槽，比如节点1保存哈希槽0和1，节点2保存哈希槽2和3")]),s._v(" "),e("div",{staticClass:"language-shell line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[s._v("redis-cli "),e("span",{pre:!0,attrs:{class:"token parameter variable"}},[s._v("-h")]),s._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[s._v("192.168")]),s._v(".1.10 –p "),e("span",{pre:!0,attrs:{class:"token number"}},[s._v("6379")]),s._v(" cluster addslots "),e("span",{pre:!0,attrs:{class:"token number"}},[s._v("0,1")]),s._v("\nredis-cli "),e("span",{pre:!0,attrs:{class:"token parameter variable"}},[s._v("-h")]),s._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[s._v("192.168")]),s._v(".1.11 –p "),e("span",{pre:!0,attrs:{class:"token number"}},[s._v("6379")]),s._v(" cluster addslots "),e("span",{pre:!0,attrs:{class:"token number"}},[s._v("2,3")]),s._v("\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br")])]),e("p",[s._v("然后在集群运行的过程中，key1和key2计算完CRC16值后，对哈希槽总个数4进行取模，再根据各自的模数结果，就可以被映射到哈希槽1（对应节点1）和哈希槽2（对应节点2）")]),s._v(" "),e("h3",{attrs:{id:"集群脑裂导致数据丢失怎么办"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#集群脑裂导致数据丢失怎么办"}},[s._v("#")]),s._v(" 集群脑裂导致数据丢失怎么办")]),s._v(" "),e("p",[s._v("什么是脑裂？\n集群的脑裂现象，这就好比一个人有两个大脑，那么到底受谁控制呢？\n那么在Redis中，集群脑裂产生数据丢失的现象是怎样的呢？")]),s._v(" "),e("p",[s._v("在Redis主从架构中，部署方式一般是一主多从，主节点提供写操作，从节点提供读操作，如果主节点的网络出问题，它与所有的从节点都失联了，但是此时的主节点和客户端的网络是正常的，这个客户端并不知道Redis内部已经出了问题，还在照样想找个失联的住系欸但写数据，此时这些数据被旧主节点缓存到了缓冲区里，因为主从节点之间的网络问题，这些数据都是无法同步给从节点的。")]),s._v(" "),e("p",[s._v("这时，哨兵发现主节点失联了，它就认为主节点挂了（但实际上主节点正常运行，只是网络出问题了），于是哨兵就会在从节点中选举出一个leader作为主节点，这时集群就有两个主节点了，脑裂出现了。")]),s._v(" "),e("p",[s._v("然后，网络突然好了，哨兵因为之前已经选举出一个新节点了，他就会把旧主节点降级为从节点，然后从节点会向新主节点请求数据同步，因为第一次同步时全量同步的方式，此时的从节点会清空自己本地的数据，然后再做全量同步。所以，之前客户端在A写入的数据就会丢失，也就是集群产生脑裂数据丢失的问题。")]),s._v(" "),e("p",[s._v("总结一句话就是：由于网络问题，集群节点之间失去联系。主从数据不同步；重新平衡选举，产生两个主服务。等网络恢复，旧主节点降为从节点，再与新主节点进行同步复制的时候，由于会从节点清空自己的缓冲区，所以导致之前客户端写入的数据丢失了。")]),s._v(" "),e("h4",{attrs:{id:"解决方案"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#解决方案"}},[s._v("#")]),s._v(" 解决方案")]),s._v(" "),e("p",[s._v("当主节点发现从节点下线或者通信超时的总数量小于阈值时候，那么禁止主节点进行写数据，直接把错误返回给客户端。")]),s._v(" "),e("p",[s._v("再Redis的配置文件中有两个参数我们可以设置：")]),s._v(" "),e("ul",[e("li",[s._v("min-slaves-to-write x，主节点必须要有至少x个从节点连接，如果小于这个数量，主节点会禁止写数据。")]),s._v(" "),e("li",[s._v("min-slaves-max-lag x,主从数据复制和同步的延迟不能超过x秒，如果超过，主节点会禁止写数据。\n我们可以把min-slaves-to-write和min-slaves-max-lag这两个配置项搭配起来使用，分别给他们设置一定的阈值，假设为N和T。")])]),s._v(" "),e("p",[s._v("这个两个配置项组合后的要求是，主库连接的从库中至少有N个从库，和主库进行数据复制时，ACK消息延迟不饿能超过T秒，否则，主库不会再接受客户端的写请求了。")]),s._v(" "),e("p",[s._v("即使原主库时假故障，它再故障期间也无法相应哨兵心跳，也不能和从库进行同步，自然也就无法和从库进行ACK确认了。这样一来，min-slaves-to-write和min-slaves-max-lag的组合要求就无法得到满足，原主库就会被限制接受客户端写请求，客户端也就不能在原主库中写入新数据了。")]),s._v(" "),e("p",[s._v("等到新主库上线时候，就只有新主库能接受和处理客户端请求，此时，新写的数据会被直接写到新主库中。而原主库会被哨兵降为从库，即使它的数据被清空了也不会有数据丢失。")])])}),[],!1,null,null,null);e.default=v.exports}}]);